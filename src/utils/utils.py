import os
import numpy as np
import pandas as pd
import re
import torch
import json
import pytorch_lightning as pl

from ast import literal_eval
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from torch import cuda
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AdamW,
    T5Tokenizer,
    get_linear_schedule_with_warmup,
    T5ForConditionalGeneration
)


class entity:
    """
    Class that should represent an entities and its attributes: label, token and the confidence of the model prediction.
    """
    def __init__(self, token, label, confidence):
        self._token=token
        self._label=label
        self._confidence=confidence

    @property
    def token(self):
        return self._token

    @property
    def label(self):
        return self._label

    @property
    def confidence(self):
        return self._confidence



def merge_same_labels(ents_list):
    """
    This function merges together two consecutive entities with the same label (in the cleaned input entity list, without subtokens, use remove_sobtokens before this):
    this should be because bert tends to consider different entities the "composed" ones, such as name-surname of palyers or the competitions.

    TODO: this seems to work well with names but has some difficulties with negotiations and dates (which are siumilar, ie: five-year contract: it is a date but also a negotiations), present them together?
    """
    r = []
    c = entity('', '', 0)
    for e in ents_list:
        if e.label == c.label and e.label != 'O':
            if r:
                r[-1] = entity(r[-1].token + ' ' + e.token, r[-1].label, max(e.confidence, r[-1].confidence))
        else:
            r.append(e)
        c = r[-1]
    return r


def merge_ents(ent1, ent2):
    """
    Function that takes two entity objects as input and return the merge of the two removing the ## in the subtokens. The result confidence at the moment is the max.
    """
    return entity(ent1.token + ent2.token[2:], ent1.label, max(ent1.confidence, ent2.confidence))


def remove_sobtokens(ents_list):
    """
    Function that clean a list of entities object: it merges the subtokens generated by bert tokenization and returns the full terms.
    """
    r = []
    for e in ents_list:
        if e.token.startswith("##"):
            if r:
                r[-1] = merge_ents(r[-1], e)
        else:
            r.append(e)
    return r

def soft_max(array):
    """
    method that computes the softmax of an array of values and return the max computed value
    """
    expo=np.exp(array) / np.sum(np.exp(array))
    return max(expo)
    

def trim_text(row):
    text=row['Text']

    while text[0] in ['b', "\"", "'", "@", " "]:
        text=text[1:]

    while text[-1] in ['b', "\"", "'", "@", " "]:
        text=text[:-2]
        
    return text

def clean_text(row):
    text=re.sub('http://\S+|https://\S+', '', row['Text'])
    text=re.sub('[!@#$]', '', text)
    text = text.replace('\n', " ")
    text = text.replace("\\n", " ")
    text= text.replace("\\", " ") 
    return text


def get_entities(model, tokenizer, text, label_list):

    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))
    inputs = tokenizer.encode(text, return_tensors="pt")
    outputs = model(inputs)[0]
    predictions = torch.argmax(outputs, dim=2).detach().numpy()[0, :]
    score_matrix = outputs.detach().numpy()
    meaningful_ent = []
    
    for i in range(0, len(predictions)):
        meaningful_ent.append(entity(tokens[i], label_list[predictions[i]], soft_max(score_matrix[0, i, :])))

    entities_to_show=[m for m in merge_same_labels(remove_sobtokens(meaningful_ent)) if m.label!='O'and len(m.token)>1]

    return [{'text': ent.token , 'label': ent.label, 'score': ent.confidence} for ent in entities_to_show]


def train(tokenizer, model, device, loader, optimizer):
    model.train()

    for _, data in enumerate(loader, 0):
        y = data['target_ids'].to(device, dtype=torch.long)
        y_ids = y[:, :-1].contiguous()
        lm_labels = y[:, 1:].clone().detach()
        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
        ids = data['table_ids'].to(device, dtype=torch.long)
        mask = data['table_mask'].to(device, dtype=torch.long)

        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)
        loss = outputs[0]


        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return (loss)


class T5Dataset(Dataset):
    """Build the dataset class"""
    
    def __init__(self, data, tokenizer, max_length):
        super().__init__()
        self._data_type='train'
        self._tokenizer=tokenizer
        self._max_length=max_length
        self._data=data
    
    def __len__(self):
        return len(self._data)
    
    def __getitem__(self, idx):
        return {"table_ids": self._data[idx]['table']['input_ids'].squeeze(),
                "table_mask": self._data[idx]['table']['attention_mask'].squeeze(),
                "target_ids": self._data[idx]['target']['input_ids'].squeeze(),
                "target_mask": self._data[idx]['target']['attention_mask'].squeeze()}
 
    def train_test(self, a):
        train_data, val_data = train_test_split(self._data, test_size=a, random_state=42)
        return T5Dataset(train_data, self._tokenizer, self._max_length), T5Dataset(val_data, self._tokenizer, self._max_length)

    @property  
    def tokenizer(self):
        return self._tokenizer


def encode_data(tokenizer, data, max_length, pad_to_max_length=True, return_tensors="pt"):
    """
    This function reads the data (the linearized table) and returns them in tokenized form.
    Target bool value indicate wheter if the target values is passed (the final sentence) or the linearized table.

    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where 
    each dictionary contains the word piece indices among other relevant inputs for training & inference
    
    TODO: Add possibility that test files do not have the target sentence (maybe do an encode prediction)
    """
    tokenized_sentence = tokenizer.batch_encode_plus(
            [data['text']], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors, truncation =True
    )
    tokenized_table = tokenizer.batch_encode_plus(
            [data['metadata_str']], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors, truncation =True
    )
        
    return {'table': tokenized_table, 'target': tokenized_sentence}

def seconds_to_hhmmss(seconds: int):
    """
    Convert the number of seconds into hh:mm:ss format
    """
    hours, seconds =  seconds // 3600, seconds % 3600
    minutes, seconds = seconds // 60, seconds % 60
    return hours, minutes, seconds


class T5Datamodule(pl.LightningDataModule):
    def __init__(self, 
                 data_dir: str,
                 tokenizer_name: str,
                 batch_size: int = 8,
                 max_length: int = 128):
        
        super().__init__()
        self._batch_size=batch_size
        self._max_length = max_length
        self._tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)
        self._data_dir = os.path.join(data_dir)
        self._data = []

        # self.train_set = []
        # self.test_set = []
    
    # def prepare_data(self):

    #     self.train_dataset =TottoDataset(train_set)
    #     self.test_dataset = TottoDataset(test_set)

    def setup(self, stage=None):

        df=pd.read_csv(self._data_dir, sep=',', index_col=0)

        data=[]
        max_length=128

        for i,row in df.iterrows():
            self._data.append(encode_data(self._tokenizer, {'metadata_str': str(row['Input String']), 'text': row['Text']}, max_length, pad_to_max_length=True, return_tensors="pt"))

        train_set, test_set = train_test_split(self._data, test_size = 0.25)
        self._train_dataset = T5Dataset(self._tokenizer, self._max_length, train_set)
        self._test_dataset = T5Dataset(self._tokenizer, self._max_length, test_set)
                
    def train_dataloader(self):
        return DataLoader(self._train_dataset, self._batch_size, shuffle=True, num_workers=16) #ADD n_workers
    
    def get_train_len(self):
        return len(self._train_dataset)
    
    def get_batch_size(self):
        return self._batch_size

    def get_pad_token_id(self):
        return self._tokenizer.pad_token_id
    
    def get_tokenizer(self):
        return self._tokenizer


class T5Module(pl.LightningModule):
    
    def __init__(self,
                 model_name: str, 
                 lr,
                 train_set: T5Datamodule,
                 n_gpus: int = 1, 
                 n_epochs: int = 5):
        
        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(model_name, return_dict=True)
        self.lr=lr
        self.n_gpus= n_gpus
        self.n_epochs=n_epochs
        self.train_dataset=train_set
        self.pad_token_id = self.train_dataset.get_pad_token_id()
        self.tokenizer = self.train_dataset.get_tokenizer()
        
    def forward(self, input_ids, attention_mask, labels= None): #Add labels=None when test phase
        
        output=self.model(input_ids=input_ids,
                          attention_mask=attention_mask,
                          labels= labels)
        
        return output.loss, output.logits
    
    def training_step(self, batch, batch_idx):
        
        input_ids=batch["table_ids"]
        attention_mask=batch["table_mask"]
        labels=batch["target_ids"]
        labels[labels[:, :] == self.pad_token_id] = -100

        loss, outputs=self(input_ids, attention_mask, labels)

        # tensorboard_logs = {'train_loss': loss}
        # tensorboard_logs = {'lr': self.opt.param_groups[0]['lr']}
        
        self.log("lr_opt", self.opt.param_groups[0]['lr'], prog_bar=True, logger=True)
        self.log("train_loss", loss, prog_bar=True, logger=True)

        self.lr_scheduler.step()
        return loss
    
    def configure_optimizers(self):

        t_total = ((self.train_dataset.get_train_len()) // (self.train_dataset.get_batch_size() * self.n_gpus))*self.n_epochs 

        self.opt = AdamW(self.parameters(), lr=self.lr)

        self.lr_scheduler = get_linear_schedule_with_warmup(
            self.opt, num_warmup_steps = int(0.25*t_total), num_training_steps=t_total # set as input
        )
        return [self.opt], [self.lr_scheduler]


def generic_train(model: T5Module, data_module: T5Datamodule, n_gpus : int, max_epochs: int, do_train = bool, output_dir: str = './model'): # ADD ARGS

    # checkpoint_callback = pl.callbacks.ModelCheckpoint(
    #     dirpath = output_dir, filename ="checkpoint", monitor="train_loss", mode="min", save_top_k=1
    # )

    train_params = dict(
        gpus = n_gpus,
        accelerator="gpu",
        max_epochs=max_epochs,
        enable_progress_bar=True,
        #progress_bar_refresh_rate=20,
        enable_checkpointing=True,
        #checkpoint_callback=checkpoint_callback,
        precision=16,
        # automatic_optimization = False
    )
    
    if n_gpus > 1:
        train_params["distributed_backend"] = "ddp"
        

    trainer = pl.Trainer(**train_params)

    if do_train:
        trainer.fit(model, data_module)
        model.model.save_pretrained(os.path.join(output_dir))
    
    return trainer